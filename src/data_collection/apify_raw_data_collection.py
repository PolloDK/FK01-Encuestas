from apify_client import ApifyClient
import signal
import pandas as pd
import os
import sys

API_TOKEN = 'apify_api_FXff9JQtt1wIKIcMBaI1xiqVQMCBte0SeAlO'
file_path = "data/raw_data.csv"

stop_scraping = False

def signal_handler(sig, frame):
    """ Maneja cuando se quiere cortar el proceso de scraping
    sin interrumpir el script
    """
    global stop_scraping
    print("\nğŸ›‘ Deteniendo el scraper... Guardando tweets recolectados hasta ahora.")
    stop_scraping = True
    
signal.signal(signal.SIGINT, signal_handler)

client = ApifyClient(API_TOKEN)

print("ğŸš€ Iniciando el scraper de tweets... Presiona Ctrl + C para detenerlo sin perder datos.")

# Definir fecha de inicio y fecha de fin.
SINCE_DATE = "2025-02-10_00:00:00_UTC" 
UNTIL_DATE = "2025-03-10_00:00:00_UTC" 

# ValidaciÃ³n cantidad de tweets previo a la extracciÃ³n
if os.path.isfile(file_path):
    df_old = pd.read_csv(file_path)
    tweets_before = len(df_old)
else:
    tweets_before = 0  # Si el archivo no existe, no hay tweets previos

print(f"ğŸ“Š Tweets antes de la actualizaciÃ³n: {tweets_before}")

# DefiniciÃ³n de parÃ¡metros para el run del actor en Apify:
run_input = {
    "searchTerms": [
        f"Boric since:{SINCE_DATE} until:{UNTIL_DATE}",  
        f"Gabriel Boric since:{SINCE_DATE} until:{UNTIL_DATE}",
        f"Presidente Boric since:{SINCE_DATE} until:{UNTIL_DATE}"
    ],
    "maxItems": 5000,
    "queryType": "Latest",
    "lang": "es",
    "filter:verified": False,
    "filter:replies": False,
    "filter:quote": False,
    "min_retweets": 0, 
    "min_faves": 0,
}

print("ğŸ”„ Ejecutando el scraper en Apify...")
run = client.actor("kaitoeasyapi~twitter-x-data-tweet-scraper-pay-per-result-cheapest").call(run_input=run_input)
if "defaultDatasetId" not in run:
    print("âŒ Error: No se encontrÃ³ un dataset en la respuesta de Apify.")
    exit()
print("âœ… Scraper ejecutado con Ã©xito. Obteniendo tweets...")

## Buscarndo tweets con criterios
tweets = []
try:
    for item in client.dataset(run["defaultDatasetId"]).iterate_items():
        if stop_scraping:  # Si el usuario presionÃ³ Ctrl + C, detenemos la recolecciÃ³n
            break
        tweets.append(item)
except Exception as e:
    print(f"âš ï¸ Error durante la recolecciÃ³n: {e}")

if not tweets:
    print("âš ï¸ No se encontraron tweets con los criterios especificados.")
    sys.exit(1)

df = pd.DataFrame(tweets)
columnas_relevantes = ["id", "full_text", "created_at", "user_screen_name", "retweet_count", "favorite_count"]
df = df[columnas_relevantes] if set(columnas_relevantes).issubset(df.columns) else df

# Se agregan tweets al archivo de raw data:
if os.path.exists(file_path):
    print("ğŸ“‚ El archivo ya existe. Agregando nuevos tweets...")
    df.to_csv(file_path, mode='a', header=False, index=False, encoding="utf-8")
else:
    print("ğŸ“‚ Creando nuevo archivo y guardando tweets...")
    df.to_csv(file_path, mode='w', header=True, index=False, encoding="utf-8")

df_updated = pd.read_csv(file_path)
tweets_after = len(df_updated)

print(f"ğŸ“Š Tweets despuÃ©s de la actualizaciÃ³n: {tweets_after}")
print(f"ğŸ“ˆ Se agregaron {tweets_after - tweets_before} nuevos tweets.")
print(f"ğŸ“‚ Tweets guardados en '{file_path}'.")
print("âœ… Proceso completado con Ã©xito.")