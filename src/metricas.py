import pandas as pd
from datetime import datetime, timedelta
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import os
import re
from tqdm import tqdm
from src.config import PROCESSED_DATA_PATH, WORDCLOUD_PATH, PREDICTIONS_PATH, FEATURES_DATASET_PATH
from src.logger import get_logger
logger = get_logger(__name__, "metricas.log")

def clasificar_sentimiento(row):
    scores = {
        "positivo": row["score_positive"],
        "negativo": row["score_negative"],
        "neutro": row["score_neutral"]
    }
    max_score = max(scores.values())

    # Manejo de empates: si hay m√°s de un score con el mismo valor m√°ximo
    candidatos = [k for k, v in scores.items() if v == max_score]
    if len(candidatos) > 1:
        return "neutro"  # asignamos neutro en caso de empate
    return candidatos[0]

def calcular_metricas():
    logger.info("Iniciando c√°lculo de m√©tricas")

    try:
        df_pred = pd.read_csv(PREDICTIONS_PATH, parse_dates=["date"])
        logger.info("Archivo de predicciones cargado.")
    except Exception as e:
        logger.error(f"No se pudo cargar predicciones_diarias.csv: {e}")
        return

    try:
        df_features = pd.read_csv(FEATURES_DATASET_PATH, parse_dates=["date"])
        negatividad_por_dia = df_features.groupby(df_features['date'].dt.date)['score_negative'].mean().reset_index()
        negatividad_por_dia.columns = ['date', 'indice_negatividad']
        negatividad_por_dia["date"] = pd.to_datetime(negatividad_por_dia["date"])
        logger.info("√çndice de negatividad calculado.")
    except Exception as e:
        logger.error(f"Error en c√°lculo de √≠ndice de negatividad: {e}")
        return

    try:
        print("Comenzando c√°lculo de % tweets negativos")
        df_raw = pd.read_csv(PROCESSED_DATA_PATH, parse_dates=["createdAt"])
        print("Base cargada")
        df_raw = df_raw.dropna(subset=["score_positive", "score_negative", "score_neutral"])
        print("se elimin√≥ NaNs")
        df_raw["date_only"] = df_raw["createdAt"].dt.date
        print(f"üîç Filas restantes antes de clasificar: {len(df_raw)}")
        
        # Vectorizado sin .apply
        scores = df_raw[["score_positive", "score_negative", "score_neutral"]]
        df_raw["sentimiento_clasificado"] = scores.idxmax(axis=1).str.replace("score_", "")

        empates = scores.eq(scores.max(axis=1), axis=0).sum(axis=1) > 1
        df_raw.loc[empates, "sentimiento_clasificado"] = "neutro"
        print("‚úÖ Clasificaci√≥n vectorizada completada")

        conteos = df_raw.groupby("date_only")["sentimiento_clasificado"].value_counts().unstack(fill_value=0).reset_index()
        conteos.columns.name = None

        # Asegurar que todas las clases est√©n presentes
        for col in ["positive", "negative", "neutral"]:
            if col not in conteos.columns:
                conteos[col] = 0

        conteos = conteos.rename(columns={
            "positive": "tweets_positivos",
            "negative": "tweets_negativos",
            "neutral": "tweets_neutros"
        })
        conteos = conteos.rename(columns={"date_only": "date"})
        conteos = conteos.drop(columns=["neutro"], errors="ignore")

        conteos["total_tweets"] = conteos[["tweets_negativos", "tweets_positivos", "tweets_neutros"]].sum(axis=1)
        conteos["porcentaje_tweets_negativos"] = conteos["tweets_negativos"] / conteos["total_tweets"]
        conteos["date"] = pd.to_datetime(conteos["date"])
        logger.info("Porcentaje de tweets negativos calculado.")
        #print("üß™ Conteos (√∫ltimas filas):")
        #print(conteos.tail())
        #print("üìä Columnas finales:", conteos.columns.tolist())
    except Exception as e:
        logger.error(f"Error en c√°lculo de % tweets negativos: {e}")
        return

    try:
        #print("üìÖ Fechas predicci√≥n:", df_pred["date"].tail())
        #print("üìÖ Fechas m√©tricas:", df_metricas["date"].tail())
        #print("üìä Columnas m√©tricas:", df_metricas.columns.tolist())
        #df_pred["date"] = pd.to_datetime(df_pred["date"]).dt.normalize()
        #conteos["date"] = pd.to_datetime(conteos["date"]).dt.normalize()
        #negatividad_por_dia["date"] = pd.to_datetime(negatividad_por_dia["date"]).dt.normalize()
        df_metricas = pd.merge(negatividad_por_dia, conteos, on="date", how="outer")
        df_actualizado = pd.merge(df_pred, df_metricas, on="date", how="left")
        df_actualizado.to_csv(PREDICTIONS_PATH, index=False, date_format="%Y-%m-%d")
        logger.info("Archivo de predicciones_diarias.csv actualizado con m√©tricas.")
    except Exception as e:
        logger.error(f"Error al actualizar archivo final: {e}")
        
def generar_wordcloud_diario():
    today = datetime.today().date()
    generar_wordcloud_para_fecha(today)


def generar_wordcloud_para_fecha(target_date):
    os.makedirs(WORDCLOUD_PATH, exist_ok=True)

    df = pd.read_csv(PROCESSED_DATA_PATH, parse_dates=["createdAt"])
    if "createdAt" not in df.columns or "text" not in df.columns:
        logger.warning("El archivo no tiene las columnas requeridas: createdAt y text")
        return

    df["createdAt"] = pd.to_datetime(df["createdAt"], errors='coerce')
    df = df.dropna(subset=["createdAt", "text"])
    df_target = df[df["createdAt"].dt.date == target_date]

    if df_target.empty:
        logger.warning(f"No hay tweets disponibles para {target_date}")
        return

    text = " ".join(df_target["text"].dropna().astype(str))
    # üîç Lista de palabras o frases a eliminar (convertidas a min√∫sculas)
    frases_excluir = ["gabriel boric", "presidente boric"]
    palabras_excluir = ["boric", "gabriel", "presidente", "chile"]

    # Eliminar frases primero
    for frase in frases_excluir:
        text = text.replace(frase, "")

    # Eliminar palabras individuales usando regex con l√≠mites de palabra
    for palabra in palabras_excluir:
        text = re.sub(rf"\b{re.escape(palabra)}\b", "", text)

    wordcloud = WordCloud(
        width=800, height=400,
        background_color='white',
        colormap='plasma',
        max_words=200
    ).generate(text)

    output_path = os.path.join(WORDCLOUD_PATH, f"wordcloud_{target_date}.png")
    wordcloud.to_file(output_path)
    logger.info(f"Wordcloud generado: {output_path}")


def generar_wordclouds_historicos():
    df = pd.read_csv(PROCESSED_DATA_PATH, parse_dates=["createdAt"])
    df = df.dropna(subset=["createdAt", "text"])
    dias_unicos = df["createdAt"].dt.date.unique()

    for i, fecha in enumerate(tqdm(sorted(dias_unicos), desc="Wordclouds")):
        generar_wordcloud_para_fecha(fecha)


def main():
    calcular_metricas()
    generar_wordcloud_diario()
    #generar_wordclouds_historicos()  # Ejecuta solo si quieres correr todos

if __name__ == "__main__":
    main()
